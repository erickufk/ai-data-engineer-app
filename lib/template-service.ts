// Template Service for generating various artifacts
import type { PipelineSpecV1 } from "./llm-service"

export class TemplateService {
  generatePostgreSQLDDL(spec: PipelineSpecV1): string {
    const target = spec.targets[0]
    const source = spec.sources[0]
    const tableName = target.ddl.table || target.entity

    return `-- ${spec.project.name} — PostgreSQL DDL
CREATE TABLE IF NOT EXISTS ${tableName} (
${source.schema.fields
  .map((field) => `  "${field.name}" ${this.mapTypeToPostgreSQL(field.type)} ${field.nullable ? "" : "NOT NULL"}`)
  .join(",\n")}
);

-- Primary/unique keys
${
  source.schema.primaryKey.length > 0
    ? `ALTER TABLE ${tableName} ADD PRIMARY KEY (${source.schema.primaryKey.map((key) => `"${key}"`).join(", ")});`
    : "-- No primary key defined"
}

-- Indexes
${target.ddl.indexes
  .map(
    (index) =>
      `CREATE INDEX IF NOT EXISTS idx_${tableName}_${index.fields[0]} ON ${tableName} (${index.fields.map((f) => `"${f}"`).join(", ")});`,
  )
  .join("\n")}`
  }

  generateClickHouseDDL(spec: PipelineSpecV1): string {
    const target = spec.targets[0]
    const source = spec.sources[0]
    const tableName = target.ddl.table || target.entity
    const engine = target.loadPolicy.dedupKeys.length > 0 ? "ReplacingMergeTree()" : "MergeTree()"

    return `-- ${spec.project.name} — ClickHouse DDL
CREATE TABLE IF NOT EXISTS ${tableName} (
${source.schema.fields.map((field) => `  \`${field.name}\` ${this.mapTypeToClickHouse(field.type)}`).join(",\n")}
)
ENGINE = ${engine}
${
  target.ddl.partitions.field
    ? `PARTITION BY to${target.ddl.partitions.granularity === "month" ? "YYYYMM" : "YYYYMMDD"}(${target.ddl.partitions.field})`
    : ""
}
ORDER BY (${target.ddl.orderBy.join(", ")});`
  }

  generateAirflowDAG(spec: PipelineSpecV1): string {
    const projectSafe = spec.project.name.toLowerCase().replace(/[^a-z0-9]/g, "_")

    return `from datetime import timedelta
from airflow import DAG
from airflow.utils.dates import days_ago

# Заглушечные операторы (реализацию читает из /config/pipeline.yaml)
from operators import ExtractOp, TransformOp, LoadOp

with DAG(
    dag_id="dag_${projectSafe}",
    schedule_interval="${spec.schedule.cron}",
    start_date=days_ago(1),
    catchup=False,
    default_args=dict(retries=${spec.schedule.retries.count}, retry_delay=timedelta(seconds=${spec.schedule.retries.delaySec})),
    tags=["ai-data-engineer"]
) as dag:
    extract = ExtractOp(task_id="extract", config_path="/config/pipeline.yaml")
    ${
      spec.transforms.length > 0
        ? `transform = TransformOp(task_id="transform", config_path="/config/pipeline.yaml")
    load = LoadOp(task_id="load", config_path="/config/pipeline.yaml")
    extract >> transform >> load`
        : `load = LoadOp(task_id="load", config_path="/config/pipeline.yaml")
    extract >> load`
    }`
  }

  generateRunScript(spec: PipelineSpecV1): string {
    return `#!/usr/bin/env bash
set -euo pipefail
echo "[AI Data Engineer] Подготовка окружения…"
python -V
echo "Смотрите конфигурацию в config/pipeline.yaml. Этот пакет не выполняет реальных коннектов."`
  }

  generateRequirementsTxt(): string {
    return `pyyaml>=6.0
jinja2>=3.1
# airflow опционально, указывается пользователем в своей среде`
  }

  generateUtilsExtract(): string {
    return `# Extract utilities
# Здесь должны быть реальные коннекторы к источникам данных
# Пример: подключение к PostgreSQL, файловым системам, API и т.д.

def extract_from_source(config):
    """
    Извлечение данных из источника согласно конфигурации
    Замените на реальную реализацию
    """
    pass`
  }

  generateUtilsTransform(): string {
    return `# Transform utilities  
# Здесь должны быть функции трансформации данных
# Пример: очистка, агрегация, обогащение данных

def apply_transforms(data, transforms_config):
    """
    Применение трансформаций к данным
    Замените на реальную реализацию
    """
    pass`
  }

  generateUtilsLoad(): string {
    return `# Load utilities
# Здесь должны быть коннекторы к целевым системам
# Пример: загрузка в PostgreSQL, ClickHouse, HDFS

def load_to_target(data, target_config):
    """
    Загрузка данных в целевую систему
    Замените на реальную реализацию
    """
    pass`
  }

  generatePipelineConfig(spec: PipelineSpecV1): string {
    return `# Pipeline Configuration for ${spec.project.name}
# Generated by AI Data Engineer v1.0

version: "${spec.version}"

project:
  name: "${spec.project.name}"
  description: "${spec.project.description}"

sources:
${spec.sources
  .map(
    (source) => `  - name: "${source.name}"
    kind: "${source.kind}"
    entity: "${source.entity}"
    format: ${source.format ? `"${source.format}"` : "null"}
    schema:
      fields:
${source.schema.fields
  .map(
    (field) => `        - name: "${field.name}"
          type: "${field.type}"
          nullable: ${field.nullable}`,
  )
  .join("\n")}
      primaryKey: [${source.schema.primaryKey.map((key) => `"${key}"`).join(", ")}]
      timeField: ${source.schema.timeField ? `"${source.schema.timeField}"` : "null"}
      encoding: ${source.schema.encoding ? `"${source.schema.encoding}"` : "null"}
      timezone: ${source.schema.timezone ? `"${source.schema.timezone}"` : "null"}
    notes: ${source.notes ? `"${source.notes}"` : "null"}`,
  )
  .join("\n")}

transforms:
${spec.transforms
  .map(
    (transform) => `  - id: "${transform.id}"
    operator: "${transform.operator}"
    params: ${JSON.stringify(transform.params, null, 6)}`,
  )
  .join("\n")}

targets:
${spec.targets
  .map(
    (target) => `  - name: "${target.name}"
    kind: "${target.kind}"
    entity: "${target.entity}"
    ddl:
      table: ${target.ddl.table ? `"${target.ddl.table}"` : "null"}
      partitions:
        type: ${target.ddl.partitions.type ? `"${target.ddl.partitions.type}"` : "null"}
        field: ${target.ddl.partitions.field ? `"${target.ddl.partitions.field}"` : "null"}
        granularity: ${target.ddl.partitions.granularity ? `"${target.ddl.partitions.granularity}"` : "null"}
      indexes:
${target.ddl.indexes
  .map(
    (index) => `        - fields: [${index.fields.map((f) => `"${f}"`).join(", ")}]
          type: ${index.type ? `"${index.type}"` : "null"}`,
  )
  .join("\n")}
      orderBy: [${target.ddl.orderBy.map((col) => `"${col}"`).join(", ")}]
    loadPolicy:
      mode: "${target.loadPolicy.mode}"
      dedupKeys: [${target.loadPolicy.dedupKeys.map((key) => `"${key}"`).join(", ")}]
      watermark:
        field: ${target.loadPolicy.watermark.field ? `"${target.loadPolicy.watermark.field}"` : "null"}
        delay: "${target.loadPolicy.watermark.delay}"`,
  )
  .join("\n")}

mappings:
${spec.mappings
  .map(
    (mapping) => `  - from: "${mapping.from}"
    to: "${mapping.to}"${mapping.transformId ? `\n    transformId: "${mapping.transformId}"` : ""}`,
  )
  .join("\n")}

schedule:
  frequency: "${spec.schedule.frequency}"
  cron: "${spec.schedule.cron}"
  slaNote: ${spec.schedule.slaNote ? `"${spec.schedule.slaNote}"` : "null"}
  retries:
    count: ${spec.schedule.retries.count}
    delaySec: ${spec.schedule.retries.delaySec}

nonFunctional: ${JSON.stringify(spec.nonFunctional)}`
  }

  generateEnvSample(spec: PipelineSpecV1): string {
    const target = spec.targets[0]

    const envContent = `# Postgres (пример)
PG_HOST=localhost
PG_PORT=5432
PG_DB=mydb
PG_USER=user
PG_PASSWORD=***   # заполните вручную

# ClickHouse (пример)
CH_HOST=localhost
CH_PORT=9000
CH_DB=analytics
CH_USER=default
CH_PASSWORD=***

# HDFS / FS
DATA_ROOT=/data

# Общие
TZ=UTC`

    return envContent
  }

  generateREADME(spec: PipelineSpecV1, reportMarkdown: string): string {
    const projectSafe = spec.project.name.toLowerCase().replace(/[^a-z0-9]/g, "_")

    return `# ${spec.project.name} — AI Data Engineer Package

## Что внутри
- /ddl — DDL для целевой СУБД
- /etl — Airflow DAG и утилиты
- /config — pipeline.yaml и .env.sample
- /scripts — скрипты запуска
- /docs — отчёты и расписание

## Минимальные требования
- Python 3.10+ (airflow необязательно для локальной сборки)
- zip, bash/sh
- (опционально) Apache Airflow 2.x

## Быстрый старт (локально)
1) Скопируйте \`.env.sample\` в \`.env\` и заполните переменные.
2) Установите зависимости:
   \`\`\`bash
   pip install -r requirements/requirements.txt
   \`\`\`
3) Ознакомьтесь с расписанием в docs/schedule.md.
4) (Опционально) Импортируйте DAG в Airflow: поместите /etl/dag_${projectSafe}.py в папку dags.
5) Выполните загрузку согласно config/pipeline.yaml и комментариям в DAG.

## Важные примечания

Пакет не содержит реальных коннекторов и секретов.

Режим загрузки: ${spec.targets[0]?.loadPolicy.mode}; ключи дедупликации: ${spec.targets[0]?.loadPolicy.dedupKeys.join(", ") || "нет"}.

Партиционирование: ${spec.targets[0]?.ddl.partitions.type || "нет"} ${spec.targets[0]?.ddl.partitions.field ? `по полю ${spec.targets[0].ddl.partitions.field}` : ""}.

---
_Сгенерировано AI Data Engineer. Версия спецификации: ${spec.version}._`
  }

  generateScheduleDoc(spec: PipelineSpecV1): string {
    return `# Расписание задач

- Частота: ${spec.schedule.frequency}
- Cron: \`${spec.schedule.cron}\`
- Ретраи: ${spec.schedule.retries.count} раз, задержка ${spec.schedule.retries.delaySec} сек.
- SLA: ${spec.schedule.slaNote || "не задана"}

## Установка в Airflow
- Установите \`schedule_interval="${spec.schedule.cron}"\` и \`retries=${spec.schedule.retries.count}\`.
- При необходимости используйте \`retry_delay=timedelta(seconds=${spec.schedule.retries.delaySec})\`.

## Советы
- Для потоковых данных используйте отдельный DAG/consumer (см. опциональные шаблоны).`
  }

  generateDesignReport(spec: PipelineSpecV1, reportMarkdown: string): string {
    return `# Design Report — ${spec.project.name}
_Сгенерировано AI Data Engineer. Версия спецификации: ${spec.version}._

${reportMarkdown}`
  }

  private mapTypeToPostgreSQL(type: string): string {
    const typeMap: Record<string, string> = {
      string: "TEXT",
      integer: "INTEGER",
      float: "DOUBLE PRECISION",
      boolean: "BOOLEAN",
      datetime: "TIMESTAMP WITH TIME ZONE",
      date: "DATE",
      json: "JSONB",
    }
    return typeMap[type.toLowerCase()] || "TEXT"
  }

  private mapTypeToClickHouse(type: string): string {
    const typeMap: Record<string, string> = {
      string: "String",
      integer: "Int64",
      float: "Float64",
      boolean: "UInt8",
      datetime: "DateTime",
      date: "Date",
      json: "String",
    }
    return typeMap[type.toLowerCase()] || "String"
  }

  generateUserPromptTemplate(fileProfile: any, projectMeta: any): string {
    const { columns, inferredTypes, timeFields, format, sampleData, sampleInfo } = fileProfile

    return `PROJECT:
- name: ${projectMeta.name}
- description: ${projectMeta.description}

FILE PROFILE:
- format: ${format}
- columns: ${columns?.join(", ") || "unknown"}
- inferred types: ${
      inferredTypes
        ? Object.entries(inferredTypes)
            .map(([col, type]) => `${col}:${type}`)
            .join(", ")
        : "unknown"
    }
- time fields: ${timeFields?.join(", ") || "none"}
- sample rows: ${sampleData?.length || 0}
${sampleInfo ? `- file size: ${this.formatFileSize(sampleInfo.originalSize)} (analyzed ${sampleInfo.percent.toFixed(1)}%)` : ""}

SAMPLE DATA (first 5 rows):
${
  sampleData
    ?.slice(0, 5)
    .map((row: any, i: number) => `Row ${i + 1}: ${JSON.stringify(row)}`)
    .join("\n") || "No sample data available"
}`
  }

  private formatFileSize(bytes: number): string {
    if (!bytes || bytes === 0) return "0 Bytes"
    const k = 1024
    const sizes = ["Bytes", "KB", "MB", "GB"]
    const i = Math.floor(Math.log(bytes) / Math.log(k))
    return Number.parseFloat((bytes / Math.pow(k, i)).toFixed(2)) + " " + sizes[i]
  }
}

export const templateService = new TemplateService()
