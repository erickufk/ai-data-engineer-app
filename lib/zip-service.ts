// ZIP Service for packaging artifacts
import JSZip from "jszip"
import type { PipelineSpecV1 } from "./llm-service"
import { templateService } from "./template-service"

export class ZipService {
  async generateProjectZip(spec: PipelineSpecV1, reportMarkdown: string): Promise<Blob> {
    const zip = new JSZip()

    // Create folder structure according to specification
    const ddlFolder = zip.folder("ddl")
    const etlFolder = zip.folder("etl")
    const configFolder = zip.folder("config")
    const scriptsFolder = zip.folder("scripts")
    const docsFolder = zip.folder("docs")

    try {
      // Generate DDL files based on target storage
      const target = spec.targets[0]
      if (target) {
        if (target.kind === "postgres") {
          ddlFolder?.file("create_tables_pg.sql", templateService.generatePostgreSQLDDL(spec))
        } else if (target.kind === "clickhouse") {
          ddlFolder?.file("create_tables_ch.sql", templateService.generateClickHouseDDL(spec))
        } else if (target.kind === "hdfs") {
          ddlFolder?.file("hdfs_structure.txt", this.generateHDFSStructure(spec))
        }
      }

      // Generate additional DDL files
      ddlFolder?.file("indexes.sql", this.generateIndexesSQL(spec))
      ddlFolder?.file("partitions.sql", this.generatePartitionsSQL(spec))

      // Generate ETL files
      const projectSlug = spec.project.name.toLowerCase().replace(/\s+/g, "_")
      etlFolder?.file(`dag_${projectSlug}.py`, templateService.generateAirflowDAG(spec))
      etlFolder?.file("utils_extractor.py", this.generateExtractorUtils(spec))
      etlFolder?.file("utils_loader.py", this.generateLoaderUtils(spec))
      etlFolder?.file("utils_transform.py", this.generateTransformUtils(spec))
      etlFolder?.file("direct_run.py", this.generateDirectRunScript(spec))

      // Generate config files
      configFolder?.file("pipeline.yaml", this.generatePipelineYAML(spec))
      configFolder?.file(".env.sample", templateService.generateEnvSample(spec))
      configFolder?.file("logging.yaml", this.generateLoggingConfig())

      // Generate scripts
      scriptsFolder?.file("run.sh", templateService.generateRunScript(spec))
      scriptsFolder?.file("setup.sh", this.generateSetupScript(spec))
      if (target?.kind === "hdfs") {
        scriptsFolder?.file("load_to_hdfs.sh", this.generateHDFSScript(spec))
      }

      // Generate documentation
      docsFolder?.file("README.md", templateService.generateREADME(spec, reportMarkdown))
      docsFolder?.file("design_report.md", reportMarkdown)
      docsFolder?.file("schedule.md", this.generateScheduleDoc(spec))
      docsFolder?.file("data_quality.md", this.generateDataQualityDoc(spec))

      // Generate root files
      zip.file("requirements.txt", this.generateRequirements(spec))
      zip.file("docker-compose.yml", this.generateDockerCompose(spec))
      zip.file(".gitignore", this.generateGitignore())

      // Generate ZIP
      return await zip.generateAsync({
        type: "blob",
        compression: "DEFLATE",
        compressionOptions: { level: 6 },
      })
    } catch (error) {
      console.error("Error generating ZIP:", error)
      throw new Error("Failed to generate project ZIP file")
    }
  }

  private generateHDFSStructure(spec: PipelineSpecV1): string {
    const target = spec.targets[0]
    const projectSlug = spec.project.name.toLowerCase().replace(/\s+/g, "_")

    return `# HDFS Directory Structure for ${spec.project.name}
# Generated by AI Data Engineer v${spec.version}

Base Path: ${target?.entity || `/data/${projectSlug}`}

Directory Structure:
${
  target?.ddl.partitions.type === "by_date"
    ? `
├── raw/
│   ├── YYYY/
│   │   ├── MM/
│   │   │   ├── DD/
│   │   │   │   └── data_files.parquet
├── processed/
│   ├── YYYY/
│   │   ├── MM/
│   │   │   ├── DD/
│   │   │   │   └── processed_files.parquet
└── archive/
    ├── YYYY/
    │   ├── MM/
    │   │   └── archived_files.parquet
`
    : `
├── raw/
│   └── data_files.parquet
├── processed/
│   └── processed_files.parquet
└── archive/
    └── archived_files.parquet
`
}

File Formats:
- Raw data: ${spec.sources[0]?.format || "CSV"}
- Processed data: Parquet (recommended)
- Compression: Snappy

Retention Policy:
${spec.nonFunctional.retention.policy || "No retention policy specified"}

Access Permissions:
- Read: data-engineers, analysts
- Write: data-engineers, etl-service
- Admin: data-admin
`
  }

  private generateIndexesSQL(spec: PipelineSpecV1): string {
    const target = spec.targets[0]
    const tableName = target?.entity || spec.project.name.toLowerCase().replace(/\s+/g, "_")
    const source = spec.sources[0]

    return `-- Indexes for ${spec.project.name}
-- Generated by AI Data Engineer v${spec.version}

${
  target?.kind === "postgres"
    ? `
-- PostgreSQL Indexes
${target.ddl.indexes
  .map(
    (index) =>
      `CREATE INDEX IF NOT EXISTS idx_${tableName}_${index.fields.join("_")} ON ${tableName} (${index.fields.join(", ")});`,
  )
  .join("\n")}

-- Time-based index (if time field exists)
${
  source?.schema.timeField
    ? `CREATE INDEX IF NOT EXISTS idx_${tableName}_${source.schema.timeField} ON ${tableName} (${source.schema.timeField});`
    : ""
}

-- Primary key index (automatically created)
-- ALTER TABLE ${tableName} ADD CONSTRAINT pk_${tableName} PRIMARY KEY (${source?.schema.primaryKey.join(", ") || "id"});
`
    : target?.kind === "clickhouse"
      ? `
-- ClickHouse Indexes (via ORDER BY)
-- Primary ordering: (${target.ddl.orderBy.join(", ")})
-- Partitioning: ${target.ddl.partitions.type === "by_date" ? `by ${target.ddl.partitions.granularity} on ${target.ddl.partitions.field}` : "none"}

-- Secondary indexes (if needed)
-- ALTER TABLE ${tableName} ADD INDEX idx_${tableName}_secondary (column_name) TYPE minmax GRANULARITY 4;
`
      : "-- No indexes for HDFS storage"
}

-- Data quality indexes
${spec.nonFunctional.dataQualityChecks
  .map((check) => {
    if (check.check === "unique") {
      return `-- Unique constraint on ${check.field}\n-- ALTER TABLE ${tableName} ADD CONSTRAINT uk_${tableName}_${check.field} UNIQUE (${check.field});`
    }
    return `-- Quality check index for ${check.field} (${check.check})`
  })
  .join("\n")}
`
  }

  private generatePartitionsSQL(spec: PipelineSpecV1): string {
    const target = spec.targets[0]
    const tableName = target?.entity || spec.project.name.toLowerCase().replace(/\s+/g, "_")

    if (target?.ddl.partitions.type !== "by_date") {
      return `-- No date-based partitioning configured for ${spec.project.name}
-- Current partitioning strategy: ${target?.ddl.partitions.type || "none"}
`
    }

    const partitionField = target.ddl.partitions.field
    const granularity = target.ddl.partitions.granularity || "day"

    return `-- Partitioning setup for ${spec.project.name}
-- Generated by AI Data Engineer v${spec.version}
-- Partition by: ${granularity} on field ${partitionField}

${
  target.kind === "postgres"
    ? `
-- PostgreSQL Partitioning
-- Enable partitioning by ${granularity}
-- ALTER TABLE ${tableName} PARTITION BY RANGE (${partitionField});

-- Create partitions for current year
${this.generatePostgreSQLPartitions(tableName, granularity)}

-- Maintenance function for automatic partition creation
CREATE OR REPLACE FUNCTION create_monthly_partitions()
RETURNS void AS $$
DECLARE
    start_date date;
    end_date date;
    partition_name text;
BEGIN
    start_date := date_trunc('month', CURRENT_DATE);
    end_date := start_date + interval '1 month';
    partition_name := '${tableName}_' || to_char(start_date, 'YYYY_MM');
    
    EXECUTE format('CREATE TABLE IF NOT EXISTS %I PARTITION OF ${tableName} FOR VALUES FROM (%L) TO (%L)',
                   partition_name, start_date, end_date);
END;
$$ LANGUAGE plpgsql;
`
    : target.kind === "clickhouse"
      ? `
-- ClickHouse Partitioning
-- Partitioning is handled in the table definition via PARTITION BY clause
-- Current partition expression: ${granularity === "month" ? "toYYYYMM" : "toYYYYMMDD"}(${partitionField})

-- Partition management queries:
-- View partitions: SELECT partition, name, active FROM system.parts WHERE table = '${tableName}';
-- Drop old partitions: ALTER TABLE ${tableName} DROP PARTITION 'partition_id';
-- Optimize partitions: OPTIMIZE TABLE ${tableName} PARTITION 'partition_id';
`
      : "-- HDFS partitioning handled via directory structure"
}
`
  }

  private generatePostgreSQLPartitions(tableName: string, granularity: string): string {
    const currentYear = new Date().getFullYear()
    const partitions = []

    if (granularity === "month") {
      for (let month = 1; month <= 12; month++) {
        const monthStr = month.toString().padStart(2, "0")
        const nextMonth = month === 12 ? "01" : (month + 1).toString().padStart(2, "0")
        const nextYear = month === 12 ? currentYear + 1 : currentYear

        partitions.push(`-- CREATE TABLE ${tableName}_${currentYear}_${monthStr} PARTITION OF ${tableName}
--     FOR VALUES FROM ('${currentYear}-${monthStr}-01') TO ('${nextYear}-${nextMonth}-01');`)
      }
    } else {
      partitions.push(`-- Example daily partitions (create as needed):
-- CREATE TABLE ${tableName}_${currentYear}_01_01 PARTITION OF ${tableName}
--     FOR VALUES FROM ('${currentYear}-01-01') TO ('${currentYear}-01-02');`)
    }

    return partitions.join("\n")
  }

  private generatePipelineYAML(spec: PipelineSpecV1): string {
    return `# Pipeline Configuration for ${spec.project.name}
# Generated by AI Data Engineer v${spec.version}

version: "${spec.version}"

project:
  name: "${spec.project.name}"
  description: "${spec.project.description}"

sources:
${spec.sources
  .map(
    (source) => `  - name: "${source.name}"
    kind: "${source.kind}"
    entity: "${source.entity}"
    format: ${source.format ? `"${source.format}"` : "null"}
    schema:
      fields:
${source.schema.fields
  .map(
    (field) => `        - name: "${field.name}"
          type: "${field.type}"
          nullable: ${field.nullable}`,
  )
  .join("\n")}
      primaryKey: [${source.schema.primaryKey.map((key) => `"${key}"`).join(", ")}]
      timeField: ${source.schema.timeField ? `"${source.schema.timeField}"` : "null"}
      encoding: ${source.schema.encoding ? `"${source.schema.encoding}"` : "null"}
      timezone: ${source.schema.timezone ? `"${source.schema.timezone}"` : "null"}
    notes: ${source.notes ? `"${source.notes}"` : "null"}`,
  )
  .join("\n")}

transforms:
${spec.transforms
  .map(
    (transform) => `  - id: "${transform.id}"
    operator: "${transform.operator}"
    params: ${JSON.stringify(transform.params, null, 6)}`,
  )
  .join("\n")}

targets:
${spec.targets
  .map(
    (target) => `  - name: "${target.name}"
    kind: "${target.kind}"
    entity: "${target.entity}"
    ddl:
      table: ${target.ddl.table ? `"${target.ddl.table}"` : "null"}
      partitions:
        type: ${target.ddl.partitions.type ? `"${target.ddl.partitions.type}"` : "null"}
        field: ${target.ddl.partitions.field ? `"${target.ddl.partitions.field}"` : "null"}
        granularity: ${target.ddl.partitions.granularity ? `"${target.ddl.partitions.granularity}"` : "null"}
      indexes:
${target.ddl.indexes
  .map(
    (index) => `        - fields: [${index.fields.map((f) => `"${f}"`).join(", ")}]
          type: ${index.type ? `"${index.type}"` : "null"}`,
  )
  .join("\n")}
      orderBy: [${target.ddl.orderBy.map((col) => `"${col}"`).join(", ")}]
    loadPolicy:
      mode: "${target.loadPolicy.mode}"
      dedupKeys: [${target.loadPolicy.dedupKeys.map((key) => `"${key}"`).join(", ")}]
      watermark:
        field: ${target.loadPolicy.watermark.field ? `"${target.loadPolicy.watermark.field}"` : "null"}
        delay: "${target.loadPolicy.watermark.delay}"`,
  )
  .join("\n")}

mappings:
${spec.mappings
  .map(
    (mapping) => `  - from: "${mapping.from}"
    to: "${mapping.to}"${mapping.transformId ? `\n    transformId: "${mapping.transformId}"` : ""}`,
  )
  .join("\n")}

schedule:
  frequency: "${spec.schedule.frequency}"
  cron: "${spec.schedule.cron}"
  slaNote: ${spec.schedule.slaNote ? `"${spec.schedule.slaNote}"` : "null"}
  retries:
    count: ${spec.schedule.retries.count}
    delaySec: ${spec.schedule.retries.delaySec}

nonFunctional:
  retention:
    policy: ${spec.nonFunctional.retention.policy ? `"${spec.nonFunctional.retention.policy}"` : "null"}
  dataQualityChecks:
${spec.nonFunctional.dataQualityChecks
  .map(
    (check) => `    - check: "${check.check}"
      field: "${check.field}"${check.min !== undefined ? `\n      min: ${typeof check.min === "string" ? `"${check.min}"` : check.min}` : ""}${check.max !== undefined ? `\n      max: ${typeof check.max === "string" ? `"${check.max}"` : check.max}` : ""}`,
  )
  .join("\n")}
  pii:
    masking: [${spec.nonFunctional.pii.masking.map((field) => `"${field}"`).join(", ")}]
    notes: ${spec.nonFunctional.pii.notes ? `"${spec.nonFunctional.pii.notes}"` : "null"}
`
  }

  private generateExtractorUtils(spec: PipelineSpecV1): string {
    return `"""
Data extraction utilities for ${spec.project.name}
Generated by AI Data Engineer v${spec.version}
"""

import pandas as pd
from typing import Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)

class DataExtractor:
    """Extract data from various sources"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
    
    def extract_from_file(self, file_path: str) -> pd.DataFrame:
        """Extract data from file"""
        logger.info(f"Extracting data from {file_path}")
        
        if file_path.endswith('.csv'):
            return pd.read_csv(file_path)
        elif file_path.endswith('.json'):
            return pd.read_json(file_path)
        else:
            raise ValueError(f"Unsupported file format: {file_path}")
    
    def extract_from_database(self, query: str) -> pd.DataFrame:
        """Extract data from database"""
        logger.info("Extracting data from database")
        # Add your database extraction logic here
        pass
    
    def validate_data(self, df: pd.DataFrame) -> bool:
        """Validate extracted data"""
        logger.info("Validating extracted data")
        
        # Basic validation
        if df.empty:
            logger.error("Extracted data is empty")
            return False
        
        # Add your validation logic here
        return True
`
  }

  private generateLoaderUtils(spec: PipelineSpecV1): string {
    return `"""
Data loading utilities for ${spec.project.name}
Generated by AI Data Engineer v${spec.version}
"""

import pandas as pd
from typing import Dict, Any
import logging

logger = logging.getLogger(__name__)

class DataLoader:
    """Load data to target storage"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.storage_type = "${spec.targets[0].kind}"
        self.load_mode = "${spec.targets[0].loadPolicy.mode}"
    
    def load_to_target(self, df: pd.DataFrame, table_name: str) -> bool:
        """Load data to target storage"""
        logger.info(f"Loading data to {self.storage_type}")
        
        try:
            if self.storage_type == "postgres":
                return self._load_to_postgresql(df, table_name)
            elif self.storage_type == "clickhouse":
                return self._load_to_clickhouse(df, table_name)
            elif self.storage_type == "hdfs":
                return self._load_to_hdfs(df, table_name)
            else:
                raise ValueError(f"Unsupported storage type: {self.storage_type}")
        except Exception as e:
            logger.error(f"Failed to load data: {e}")
            return False
    
    def _load_to_postgresql(self, df: pd.DataFrame, table_name: str) -> bool:
        """Load data to PostgreSQL"""
        # Add PostgreSQL loading logic
        logger.info(f"Loading to PostgreSQL table: {table_name}")
        return True
    
    def _load_to_clickhouse(self, df: pd.DataFrame, table_name: str) -> bool:
        """Load data to ClickHouse"""
        # Add ClickHouse loading logic
        logger.info(f"Loading to ClickHouse table: {table_name}")
        return True
    
    def _load_to_hdfs(self, df: pd.DataFrame, table_name: str) -> bool:
        """Load data to HDFS"""
        # Add HDFS loading logic
        logger.info(f"Loading to HDFS path: {table_name}")
        return True
`
  }

  private generateTransformUtils(spec: PipelineSpecV1): string {
    const transformNodes = spec.transforms

    return `"""
Data transformation utilities for ${spec.project.name}
Generated by AI Data Engineer v${spec.version}
"""

import pandas as pd
from typing import Dict, Any, List
import logging

logger = logging.getLogger(__name__)

class DataTransformer:
    """Apply transformations to data"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
    
    def apply_transformations(self, df: pd.DataFrame) -> pd.DataFrame:
        """Apply all configured transformations"""
        logger.info("Applying data transformations")
        
        result_df = df.copy()
        
        # Apply transformations in order
        ${transformNodes
          .map(
            (node) => `
        # ${node.operator}: ${JSON.stringify(node.params)}
        result_df = self.${node.operator?.toLowerCase() || "transform"}(result_df, ${JSON.stringify(node.params)})`,
          )
          .join("")}
        
        return result_df
    
    def filter(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """Apply filter transformation"""
        condition = config.get('condition', '')
        if condition:
            logger.info(f"Applying filter: {condition}")
            # Convert SQL-like condition to pandas query
            # This is a simplified example
            return df.query(condition)
        return df
    
    def project(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """Apply project transformation (select columns)"""
        columns = config.get('columns', '')
        if columns:
            column_list = [col.strip() for col in columns.split(',')]
            logger.info(f"Selecting columns: {column_list}")
            return df[column_list]
        return df
    
    def aggregate(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """Apply aggregation transformation"""
        group_by = config.get('groupBy', '')
        aggregations = config.get('aggregations', '')
        
        if group_by and aggregations:
            logger.info(f"Aggregating by {group_by}: {aggregations}")
            # This is a simplified example
            # In practice, you'd parse the aggregation expressions
            return df.groupby(group_by.split(',')).agg({'*': 'count'})
        return df
    
    def deduplicate(self, df: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
        """Apply deduplication transformation"""
        keys = config.get('keys', '')
        strategy = config.get('strategy', 'first')
        
        if keys:
            key_list = [key.strip() for key in keys.split(',')]
            logger.info(f"Deduplicating on keys: {key_list}")
            return df.drop_duplicates(subset=key_list, keep=strategy)
        return df
`
  }

  private generateHDFSScript(spec: PipelineSpecV1): string {
    return `#!/bin/bash
# HDFS loading script for ${spec.project.name}
# Generated by AI Data Engineer v${spec.version}

set -e

echo "Loading data to HDFS..."

# Load environment variables
if [ -f .env ]; then
    export $(cat .env | xargs)
fi

# Check HDFS connection
if ! hdfs dfsadmin -report > /dev/null 2>&1; then
    echo "Error: HDFS is not available"
    exit 1
fi

# Create directory structure
hdfs dfs -mkdir -p /data/${spec.project.name.toLowerCase().replace(/\s+/g, "_")}

# Load data files
echo "Uploading data files to HDFS..."
hdfs dfs -put data/* /data/${spec.project.name.toLowerCase().replace(/\s+/g, "_")}/

echo "HDFS loading completed!"
`
  }

  private generateScheduleDoc(spec: PipelineSpecV1): string {
    return `# Scheduling Documentation for ${spec.project.name}

## Current Schedule

- **Frequency**: ${spec.schedule.frequency}
- **Cron Expression**: \`${spec.schedule.cron}\`
- **Load Mode**: ${spec.targets[0].loadPolicy.mode}

## Schedule Details

${
  spec.schedule.frequency === "hourly"
    ? `
### Hourly Execution
- Runs every hour at minute 0
- Suitable for real-time or near-real-time processing
- Monitor for resource usage during peak hours
`
    : spec.schedule.frequency === "daily"
      ? `
### Daily Execution  
- Runs once per day at 2:00 AM
- Good for batch processing of daily data
- Allows for overnight processing window
`
      : `
### Weekly Execution
- Runs once per week on Sunday at 2:00 AM
- Suitable for weekly reporting and aggregations
- Lower resource usage, good for large datasets
`
}

## Monitoring

- Check Airflow UI for execution status
- Monitor logs in \`/var/log/airflow/\`
- Set up alerts for failed executions

## Customization

To change the schedule:

1. Edit \`config/pipeline.yaml\`
2. Update the cron expression
3. Redeploy the Airflow DAG

## Cron Expression Guide

\`\`\`
* * * * *
│ │ │ │ │
│ │ │ │ └── Day of week (0-7, Sunday = 0 or 7)
│ │ │ └──── Month (1-12)
│ │ └────── Day of month (1-31)
│ └──────── Hour (0-23)
└────────── Minute (0-59)
\`\`\`

## Examples

- Every hour: \`0 * * * *\`
- Daily at 2 AM: \`0 2 * * *\`
- Weekly on Sunday: \`0 2 * * 0\`
- Every 15 minutes: \`*/15 * * * *\`
`
  }

  private generateDirectRunScript(spec: PipelineSpecV1): string {
    return `"""
Direct execution script for ${spec.project.name}
Alternative to Airflow for local testing
Generated by AI Data Engineer v${spec.version}
"""

import sys
import os
import yaml
import logging
from datetime import datetime

# Add project root to path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from utils_extractor import DataExtractor
from utils_transform import DataTransformer  
from utils_loader import DataLoader

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def load_config():
    """Load pipeline configuration"""
    config_path = os.path.join(os.path.dirname(__file__), '..', 'config', 'pipeline.yaml')
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def main():
    """Main execution function"""
    logger.info("Starting direct pipeline execution for ${spec.project.name}")
    
    try:
        # Load configuration
        config = load_config()
        logger.info("Configuration loaded successfully")
        
        # Initialize components
        extractor = DataExtractor(config)
        transformer = DataTransformer(config)
        loader = DataLoader(config)
        
        # Extract data
        logger.info("Starting data extraction...")
        source_config = config['sources'][0]
        if source_config['kind'] == 'file':
            df = extractor.extract_from_file(source_config['entity'])
        else:
            df = extractor.extract_from_database("SELECT * FROM " + source_config['entity'])
        
        logger.info(f"Extracted {len(df)} records")
        
        # Validate extracted data
        if not extractor.validate_data(df):
            raise ValueError("Data validation failed")
        
        # Transform data
        logger.info("Starting data transformation...")
        transformed_df = transformer.apply_transformations(df)
        logger.info(f"Transformed data: {len(transformed_df)} records")
        
        # Load data
        logger.info("Starting data loading...")
        target_config = config['targets'][0]
        success = loader.load_to_target(transformed_df, target_config['entity'])
        
        if success:
            logger.info("Pipeline execution completed successfully")
        else:
            raise RuntimeError("Data loading failed")
            
    except Exception as e:
        logger.error(f"Pipeline execution failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
`
  }

  private generateLoggingConfig(): string {
    return `# Logging configuration for AI Data Engineer pipeline
version: 1
disable_existing_loggers: false

formatters:
  standard:
    format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
  detailed:
    format: '%(asctime)s - %(name)s - %(levelname)s - %(module)s - %(funcName)s - %(message)s'

handlers:
  console:
    class: logging.StreamHandler
    level: INFO
    formatter: standard
    stream: ext://sys.stdout
  
  file:
    class: logging.handlers.RotatingFileHandler
    level: DEBUG
    formatter: detailed
    filename: /var/log/pipeline/pipeline.log
    maxBytes: 10485760  # 10MB
    backupCount: 5

loggers:
  '':
    level: INFO
    handlers: [console, file]
    propagate: false
  
  airflow:
    level: WARNING
    handlers: [console, file]
    propagate: false
`
  }

  private generateSetupScript(spec: PipelineSpecV1): string {
    const target = spec.targets[0]

    return `#!/bin/bash
# Setup script for ${spec.project.name}
# Generated by AI Data Engineer v${spec.version}

set -e

echo "Setting up ${spec.project.name} pipeline..."

# Create necessary directories
mkdir -p logs
mkdir -p data
mkdir -p temp

# Check if .env exists
if [ ! -f .env ]; then
    echo "Creating .env from template..."
    cp config/.env.sample .env
    echo "Please edit .env with your actual configuration values"
fi

# Install Python dependencies
echo "Installing Python dependencies..."
pip install -r requirements.txt

# Setup database (if applicable)
${
  target?.kind === "postgres"
    ? `
echo "Setting up PostgreSQL database..."
if command -v psql &> /dev/null; then
    echo "Running DDL scripts..."
    psql -f ddl/create_tables_pg.sql
    psql -f ddl/indexes.sql
    psql -f ddl/partitions.sql
    echo "PostgreSQL setup completed"
else
    echo "Warning: psql not found. Please run DDL scripts manually."
fi
`
    : target?.kind === "clickhouse"
      ? `
echo "Setting up ClickHouse database..."
if command -v clickhouse-client &> /dev/null; then
    echo "Running DDL scripts..."
    clickhouse-client --multiquery < ddl/create_tables_ch.sql
    echo "ClickHouse setup completed"
else
    echo "Warning: clickhouse-client not found. Please run DDL scripts manually."
fi
`
      : target?.kind === "hdfs"
        ? `
echo "Setting up HDFS directories..."
if command -v hdfs &> /dev/null; then
    hdfs dfs -mkdir -p ${target.entity}
    echo "HDFS setup completed"
else
    echo "Warning: hdfs command not found. Please create directories manually."
fi
`
        : ""
}

# Setup Airflow (if available)
if command -v airflow &> /dev/null; then
    echo "Setting up Airflow..."
    
    # Initialize Airflow database
    airflow db init
    
    # Copy DAG to Airflow
    if [ -n "$AIRFLOW_HOME" ]; then
        cp etl/dag_*.py $AIRFLOW_HOME/dags/
        echo "DAG copied to Airflow"
    else
        echo "Warning: AIRFLOW_HOME not set. Please copy DAG manually."
    fi
else
    echo "Airflow not found. You can run the pipeline directly using:"
    echo "python etl/direct_run.py"
fi

echo "Setup completed! Next steps:"
echo "1. Edit .env with your configuration"
echo "2. Test the pipeline: ./scripts/run.sh"
echo "3. Monitor execution in Airflow UI (if using Airflow)"
`
  }

  private generateDataQualityDoc(spec: PipelineSpecV1): string {
    return `# Data Quality Documentation for ${spec.project.name}

## Overview

This document outlines the data quality checks and validation rules implemented in the pipeline.

## Data Quality Checks

${
  spec.nonFunctional.dataQualityChecks.length > 0
    ? spec.nonFunctional.dataQualityChecks
        .map(
          (check) => `
### ${check.check.toUpperCase()} Check on ${check.field}

- **Type**: ${check.check}
- **Field**: ${check.field}
${check.min !== undefined ? `- **Minimum**: ${check.min}` : ""}
${check.max !== undefined ? `- **Maximum**: ${check.max}` : ""}

**Implementation**: 
${
  check.check === "not_null"
    ? "Validates that the field contains no null values"
    : check.check === "unique"
      ? "Ensures all values in the field are unique"
      : check.check === "range"
        ? `Validates that values fall within the specified range (${check.min} to ${check.max})`
        : "Custom validation logic"
}
`,
        )
        .join("\n")
    : "No specific data quality checks configured."
}

## PII Handling

${
  spec.nonFunctional.pii.masking.length > 0
    ? `
### Masked Fields
The following fields contain personally identifiable information and are masked:

${spec.nonFunctional.pii.masking.map((field) => `- **${field}**: Masked using appropriate anonymization technique`).join("\n")}

### Notes
${spec.nonFunctional.pii.notes || "No additional PII notes provided."}
`
    : "No PII fields identified in the dataset."
}

## Data Validation Process

1. **Pre-processing Validation**
   - Schema validation against expected structure
   - Data type validation
   - Required field presence check

2. **Business Rule Validation**
   - Custom business logic validation
   - Cross-field validation rules
   - Referential integrity checks

3. **Post-processing Validation**
   - Record count validation
   - Data completeness check
   - Statistical validation (if applicable)

## Error Handling

- **Validation Failures**: Records failing validation are logged and quarantined
- **Data Quality Alerts**: Automated alerts for quality threshold breaches
- **Recovery Process**: Manual review and correction process for failed records

## Monitoring and Reporting

- **Quality Metrics**: Tracked in pipeline monitoring dashboard
- **Quality Reports**: Generated after each pipeline execution
- **Trend Analysis**: Historical quality metrics for trend identification

## Configuration

Data quality rules are configured in:
- \`config/pipeline.yaml\` - Main quality check definitions
- \`etl/utils_transform.py\` - Implementation of validation logic
- Environment variables for quality thresholds

## Maintenance

- Review quality rules quarterly
- Update validation logic as business requirements change
- Monitor false positive rates and adjust thresholds accordingly
`
  }

  private generateRequirements(spec: PipelineSpecV1): string {
    const target = spec.targets[0]

    return `# Python requirements for ${spec.project.name}
# Generated by AI Data Engineer v${spec.version}

# Core dependencies
apache-airflow==2.8.0
pandas==2.1.4
numpy==1.24.4
pyyaml==6.0.1
python-dotenv==1.0.0

# Database drivers
${target?.kind === "postgres" ? "psycopg2-binary==2.9.9  # PostgreSQL" : ""}
${target?.kind === "clickhouse" ? "clickhouse-driver==0.2.7  # ClickHouse" : ""}
${target?.kind === "hdfs" ? "hdfs3==0.3.1  # HDFS" : ""}

# Data processing
sqlalchemy==2.0.25
pyarrow==14.0.2  # For Parquet support

# Utilities
requests==2.31.0
click==8.1.7
tqdm==4.66.1

# Data validation
great-expectations==0.18.8
pydantic==2.5.3

# Monitoring and logging
prometheus-client==0.19.0
structlog==23.2.0

# Development and testing
pytest==7.4.4
pytest-cov==4.1.0
black==23.12.1
flake8==7.0.0
mypy==1.8.0

# Security
cryptography==41.0.8
`
  }

  private generateDockerCompose(spec: PipelineSpecV1): string {
    const target = spec.targets[0]
    const projectSlug = spec.project.name.toLowerCase().replace(/\s+/g, "_")

    return `# Docker Compose for ${spec.project.name}
# Generated by AI Data Engineer v${spec.version}

version: '3.8'

services:
  # Airflow services
  airflow-webserver:
    image: apache/airflow:2.8.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    volumes:
      - ./etl:/opt/airflow/dags
      - ./config:/opt/airflow/config
      - ./logs:/opt/airflow/logs
    ports:
      - "8080:8080"
    depends_on:
      - postgres
    command: webserver

  airflow-scheduler:
    image: apache/airflow:2.8.0
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    volumes:
      - ./etl:/opt/airflow/dags
      - ./config:/opt/airflow/config
      - ./logs:/opt/airflow/logs
    depends_on:
      - postgres
    command: scheduler

${
  target?.kind === "postgres"
    ? `
  # PostgreSQL database
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./ddl:/docker-entrypoint-initdb.d
    ports:
      - "5432:5432"

  # Application PostgreSQL (if different from Airflow)
  app-postgres:
    image: postgres:15
    environment:
      - POSTGRES_USER=\${POSTGRES_USER:-datauser}
      - POSTGRES_PASSWORD=\${POSTGRES_PASSWORD:-datapass}
      - POSTGRES_DB=\${POSTGRES_DB:-${projectSlug}}
    volumes:
      - app_postgres_data:/var/lib/postgresql/data
    ports:
      - "5433:5432"
`
    : ""
}

${
  target?.kind === "clickhouse"
    ? `
  # ClickHouse database
  clickhouse:
    image: clickhouse/clickhouse-server:23.12
    environment:
      - CLICKHOUSE_DB=\${CLICKHOUSE_DB:-default}
      - CLICKHOUSE_USER=\${CLICKHOUSE_USER:-default}
      - CLICKHOUSE_PASSWORD=\${CLICKHOUSE_PASSWORD:-}
    volumes:
      - clickhouse_data:/var/lib/clickhouse
      - ./ddl:/docker-entrypoint-initdb.d
    ports:
      - "8123:8123"
      - "9000:9000"
`
    : ""
}

  # Redis for caching (optional)
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
${target?.kind === "postgres" ? "  app_postgres_data:" : ""}
${target?.kind === "clickhouse" ? "  clickhouse_data:" : ""}
  redis_data:

networks:
  default:
    name: ${projectSlug}_network
`
  }

  private generateGitignore(): string {
    return `# AI Data Engineer Pipeline - Generated .gitignore

# Environment variables
.env
.env.local
.env.*.local

# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual environments
venv/
env/
ENV/
env.bak/
venv.bak/

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# Logs
logs/
*.log
*.log.*

# Data files (add specific patterns as needed)
data/
temp/
*.csv
*.json
*.parquet
*.avro

# Airflow
airflow.cfg
airflow.db
airflow-webserver.pid
standalone_admin_password.txt

# Database
*.db
*.sqlite
*.sqlite3

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Jupyter Notebooks
.ipynb_checkpoints/
*.ipynb

# Testing
.pytest_cache/
.coverage
htmlcov/
.tox/

# Docker
.dockerignore
docker-compose.override.yml
`
  }
}

export const zipService = new ZipService()
